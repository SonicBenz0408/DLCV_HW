{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DLCV-HW3-1",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xxu_zFqjWzP"
      },
      "source": [
        "! git clone https://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@github.com/DLCV-Fall-2021/hw3-SonicBenz0408.git\n",
        "! bash ./hw3-SonicBenz0408/get_dataset.sh\n",
        "! pip install -r /content/hw3-SonicBenz0408/requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn3DxvY5lS8O"
      },
      "source": [
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def same_seeds(seed):\n",
        "    # Python built-in random module\n",
        "    random.seed(seed)\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "    # Torch\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "same_seeds(7414)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb2Y4Xi1nfVk"
      },
      "source": [
        "from pytorch_pretrained_vit import ViT\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils import spectral_norm\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import timm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ViMMBNdnuJY"
      },
      "source": [
        "class ImgDataset(Dataset):\n",
        "    def __init__(self, path, fnames, transform):\n",
        "        self.path = path\n",
        "        self.fnames = fnames\n",
        "        self.transform = transform\n",
        "        self.num_samples = len(self.fnames)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        slice_point = self.fnames[idx].find(\"_\")\n",
        "        label = int(self.fnames[idx][:slice_point])\n",
        "        fname = os.path.join(self.path, self.fnames[idx])\n",
        "        img = Image.open(fname).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBo7OYMzn8fA"
      },
      "source": [
        "t_tfm = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.3),\n",
        "    transforms.RandomAffine(degrees=20, scale=(0.7, 1.3)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "v_tfm = transforms.Compose([\n",
        "    transforms.Resize((384, 384)),\n",
        "    #transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "    #transforms.ColorJitter(brightness=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "train_path = \"/content/hw3_data/p1_data/train/\"\n",
        "total_fnames = os.listdir(train_path)\n",
        "random.shuffle(total_fnames)\n",
        "train_fnames = total_fnames[len(total_fnames)//10:]\n",
        "val_fnames = total_fnames[:len(total_fnames)//10]\n",
        "\n",
        "train_fnames.sort()\n",
        "val_fnames.sort()\n",
        "\n",
        "train_set = ImgDataset(train_path, train_fnames, t_tfm)\n",
        "val_set = ImgDataset(train_path, val_fnames, v_tfm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmHtAHdQuvgk"
      },
      "source": [
        "images = [(train_set[i][0]+1)/2 for i in range(100)]\n",
        "grid_img = torchvision.utils.make_grid(images, nrow=10)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(grid_img.permute(1, 2, 0))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSSui8AOA3d_"
      },
      "source": [
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "# from package: transformers\n",
        "\n",
        "def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps, last_epoch=-1):\n",
        "\n",
        "    def lr_lambda(current_step: int):\n",
        "        if current_step < num_warmup_steps:\n",
        "            return float(current_step) / float(max(1, num_warmup_steps))\n",
        "        return max(\n",
        "            0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps))\n",
        "        )\n",
        "\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GkjQ7IgldON"
      },
      "source": [
        "import timm\n",
        "all_pretrained_models_available = timm.list_models(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1DuGkcmlfsS"
      },
      "source": [
        "print(all_pretrained_models_available)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFuATu6Tue6P"
      },
      "source": [
        "# Training hyperparameters\n",
        "batch_size = 32\n",
        "lr = 1e-4\n",
        "n_epoch = 100\n",
        "\n",
        "ckpt_dir = os.path.join(\"/content/\", 'checkpoints')\n",
        "os.makedirs(ckpt_dir, exist_ok=True)\n",
        "\n",
        "# Model\n",
        "#model = ViT('L_32', pretrained=True, patches=14, num_classes=37).cuda()\n",
        "model = timm.create_model(\"vit_tiny_patch16_384\", pretrained=True, num_classes=37).cuda()\n",
        "# Loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_set, batch_size=1, shuffle=True, num_workers=2)\n",
        "sch = torch.optim.lr_scheduler.LinearLR(opt, 1.0, 0.01, total_iters=len(train_loader)*10)\n",
        "#sch = get_linear_schedule_with_warmup(opt, len(train_loader) * 2 , len(train_loader) * (n_epoch))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyvwZPwgsZrY"
      },
      "source": [
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEvTswTxvLZu"
      },
      "source": [
        "loss_list = []\n",
        "acc_list = []\n",
        "best_acc = 0.\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "    model.train()    \n",
        "    for imgs, labels in train_loader:\n",
        "        imgs, labels = imgs.cuda(), labels.cuda()\n",
        "        \n",
        "        logits = model(imgs)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        opt.step()\n",
        "        sch.step()\n",
        "        #print(sch.get_lr())\n",
        "    \n",
        "    # validation\n",
        "    model.eval()\n",
        "    val_loss = 0.\n",
        "    acc = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs, labels = imgs.cuda(), labels.cuda()\n",
        "            \n",
        "            logits = model(imgs)\n",
        "            val_loss += criterion(logits, labels).float()\n",
        "            preds = logits.argmax(dim=-1)\n",
        "            if preds == labels:\n",
        "                acc += 1\n",
        "            \n",
        "    val_loss /= len(val_loader)\n",
        "    acc /= len(val_loader)\n",
        "    loss_list.append(val_loss)\n",
        "    acc_list.append(acc)\n",
        "    print(f'epoch {epoch+1}: acc={acc:.4f}, loss={val_loss:.4f}')\n",
        "\n",
        "    if(acc > best_acc):\n",
        "        best_acc = acc\n",
        "        print(\"save best model with acc =\", best_acc)\n",
        "        torch.save(model.state_dict(), os.path.join(ckpt_dir, \"model.ckpt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZc1Y-0OGQzu"
      },
      "source": [
        "torch.save(model.state_dict(), os.path.join(ckpt_dir, \"model9406.ckpt\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXV7DErnH-4g"
      },
      "source": [
        "test_path = \"/content/hw3_data/p1_data/val/\"\n",
        "test_fnames = os.listdir(test_path)\n",
        "\n",
        "test_fnames.sort()\n",
        "\n",
        "test_set = ImgDataset(test_path, test_fnames, v_tfm)\n",
        "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "model = timm.create_model(\"vit_small_patch16_384\", num_classes=37).cuda()\n",
        "model.load_state_dict(torch.load(\"/content/model.ckpt\"))\n",
        "model.eval()\n",
        "hit = 0\n",
        "for img, label in test_loader:\n",
        "    img, label = img.cuda(), label.cuda()\n",
        "            \n",
        "    logits = model(img)\n",
        "    pred = logits.argmax(dim=-1).float().item()\n",
        "    #print(pred, label)\n",
        "    if pred == label:\n",
        "        hit += 1\n",
        "    #else:\n",
        "        #print(\"label:\", label, \"pred:\", pred)\n",
        "acc = hit / len(test_loader)\n",
        "print(\"acc=\", acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_path = \"/content/hw3_data/p1_data/val/\"\n",
        "test_fnames = os.listdir(test_path)\n",
        "\n",
        "test_fnames.sort()\n",
        "\n",
        "test_set = ImgDataset(test_path, test_fnames, v_tfm)\n",
        "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "model = timm.create_model(\"vit_small_patch16_384\", num_classes=37).cuda()\n",
        "model.load_state_dict(torch.load(\"/content/model.ckpt\"))\n",
        "model.eval()\n",
        "pred_list = []\n",
        "for img, label in test_loader:\n",
        "    img, label = img.cuda(), label.cuda()\n",
        "    logits = model(img)\n",
        "    pred = logits.argmax(dim=-1).int().item()\n",
        "    pred_list.append(pred)"
      ],
      "metadata": {
        "id": "8nDXJF06gGYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/pred.csv\", \"w\") as file:\n",
        "    file.write(\"filename,label\\n\")\n",
        "    for i in range(len(pred_list)):\n",
        "        file.write(test_fnames[i]+\",\"+str(pred_list[i])+\"\\n\")"
      ],
      "metadata": {
        "id": "0KVH-tA0gdNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUv6kb0-nxx5"
      },
      "source": [
        "from types import MethodType\n",
        "def new_forward_features(self, x):\n",
        "    x = self.patch_embed(x)\n",
        "    cls_token = self.cls_token.expand(x.shape[0], -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
        "    if self.dist_token is None:\n",
        "        x = torch.cat((cls_token, x), dim=1)\n",
        "    else:\n",
        "        x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
        "    x = self.pos_drop(x + self.pos_embed)\n",
        "    x = self.blocks(x)\n",
        "    return x\n",
        "\n",
        "def new_forward(self, x):\n",
        "    x = self.forward_features(x)\n",
        "    return x\n",
        "    #return self.pos_embed\n",
        "def block_forward(self, x):\n",
        "    x = self.attn(self.norm1(x))\n",
        "    return x\n",
        "def attn_forward(self, x):\n",
        "    B, N, C = x.shape\n",
        "    qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "    q, k, v = qkv.unbind(0)   # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "    attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "    attn = attn.softmax(dim=-1)\n",
        "    attn = self.attn_drop(attn)\n",
        "\n",
        "    return attn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ormR4y6XHq59"
      },
      "source": [
        "from sklearn.metrics import pairwise_distances\n",
        "def cosine_similarity(a, b):\n",
        "    # Compute cosine similarity between two numpy vectors a and b\n",
        "    inner_product = np.inner(a, b)\n",
        "    #print(inner_product)\n",
        "    return inner_product / (np.linalg.norm(a) * np.linalg.norm(b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifuW_vjQqY42"
      },
      "source": [
        "test_path = \"/content/hw3_data/p1_data/val/\"\n",
        "test_fnames = os.listdir(test_path)\n",
        "\n",
        "test_fnames.sort()\n",
        "\n",
        "test_set = ImgDataset(test_path, test_fnames, v_tfm)\n",
        "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "model = timm.create_model(\"vit_small_patch16_384\", num_classes=37).cuda()\n",
        "model.forward = MethodType(new_forward, model)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Hw3/model.ckpt\"))\n",
        "model.eval()\n",
        "hit = 0\n",
        "for img, label in test_loader:\n",
        "    img, label = img.cuda(), label.cuda()\n",
        "            \n",
        "    embed = model(img)\n",
        "    break\n",
        "    #else:\n",
        "        #print(\"label:\", label, \"pred:\", pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xPS_Yej5fOF"
      },
      "source": [
        "from PIL import Image, ImageDraw\n",
        "#reference:https://nbviewer.org/github/luo3300612/Visualizer/blob/main/demo.ipynb\n",
        "def cls_padding(image, mask, cls_weight, grid_size):\n",
        "    if not isinstance(grid_size, tuple):\n",
        "        grid_size = (grid_size, grid_size)\n",
        "        \n",
        "    image = np.array(image)\n",
        "\n",
        "    H, W = image.shape[:2]\n",
        "    delta_H = int(H/grid_size[0])\n",
        "    delta_W = int(W/grid_size[1])\n",
        "    \n",
        "    padding_w = delta_W\n",
        "    padding_h = H\n",
        "    padding = np.ones_like(image) * 255\n",
        "    padding = padding[:padding_h, :padding_w]\n",
        "    \n",
        "    padded_image = np.hstack((padding,image))\n",
        "    padded_image = Image.fromarray(padded_image)\n",
        "    draw = ImageDraw.Draw(padded_image)\n",
        "    draw.text((int(delta_W/4),int(delta_H/4)),'CLS', fill=(0,0,0)) # PIL.Image.size = (W,H) not (H,W)\n",
        "\n",
        "    mask = mask / max(np.max(mask),cls_weight)\n",
        "    cls_weight = cls_weight / max(np.max(mask),cls_weight)\n",
        "    \n",
        "    if len(padding.shape) == 3:\n",
        "        padding = padding[:,:,0]\n",
        "        padding[:,:] = np.min(mask)\n",
        "    mask_to_pad = np.ones((1,1)) * cls_weight\n",
        "    mask_to_pad = Image.fromarray(mask_to_pad)\n",
        "    mask_to_pad = mask_to_pad.resize((delta_W, delta_H))\n",
        "    mask_to_pad = np.array(mask_to_pad)\n",
        "\n",
        "    padding[:delta_H,  :delta_W] = mask_to_pad\n",
        "    padded_mask = np.hstack((padding, mask))\n",
        "    padded_mask = padded_mask\n",
        "    \n",
        "    meta_mask = np.zeros((padded_mask.shape[0], padded_mask.shape[1],4))\n",
        "    meta_mask[delta_H:,0: delta_W, :] = 1 \n",
        "    \n",
        "    return padded_image, padded_mask, meta_mask\n",
        "def visualize_grid_to_grid_with_cls(att_map, grid_index, image, grid_size=14, alpha=0.6):\n",
        "    if not isinstance(grid_size, tuple):\n",
        "        grid_size = (grid_size, grid_size)\n",
        "    \n",
        "    attention_map = att_map[grid_index]\n",
        "    cls_weight = attention_map[0]\n",
        "    \n",
        "    mask = attention_map[1:].reshape(grid_size[0], grid_size[1])\n",
        "    mask = Image.fromarray(mask).resize((image.size))\n",
        "    \n",
        "    padded_image ,padded_mask, meta_mask = cls_padding(image, mask, cls_weight, grid_size)\n",
        "    \n",
        "    if grid_index != 0: # adjust grid_index since we pad our image\n",
        "        grid_index = grid_index + (grid_index-1) // grid_size[1]\n",
        "        \n",
        "    grid_image = highlight_grid(padded_image, [grid_index], (grid_size[0], grid_size[1]+1))\n",
        "    \n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10,7))\n",
        "    fig.tight_layout()\n",
        "    \n",
        "    ax[0].imshow(grid_image)\n",
        "    ax[0].axis('off')\n",
        "    \n",
        "    ax[1].imshow(grid_image)\n",
        "    ax[1].imshow(padded_mask, alpha=alpha, cmap='rainbow')\n",
        "    ax[1].imshow(meta_mask)\n",
        "    ax[1].axis('off')\n",
        "def highlight_grid(image, grid_indexes, grid_size=14):\n",
        "    if not isinstance(grid_size, tuple):\n",
        "        grid_size = (grid_size, grid_size)\n",
        "    \n",
        "    W, H = image.size\n",
        "    h = H / grid_size[0]\n",
        "    w = W / grid_size[1]\n",
        "    image = image.copy()\n",
        "    for grid_index in grid_indexes:\n",
        "        x, y = np.unravel_index(grid_index, (grid_size[0], grid_size[1]))\n",
        "        a= ImageDraw.ImageDraw(image)\n",
        "        a.rectangle([(y*w,x*h),(y*w+w,x*h+h)],fill =None,outline ='red',width =2)\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAR8MrCqtbM7"
      },
      "source": [
        "pic_1_path = \"/content/hw3_data/p1_data/val/26_5064.jpg\"\n",
        "pic_1 = v_tfm(Image.open(pic_1_path).convert(\"RGB\"))\n",
        "pic_1 = pic_1.reshape((1, pic_1.shape[0], pic_1.shape[1], pic_1.shape[2])).cuda()\n",
        "pic_2_path = \"/content/hw3_data/p1_data/val/29_4718.jpg\"\n",
        "pic_2 = v_tfm(Image.open(pic_2_path).convert(\"RGB\"))\n",
        "pic_2 = pic_2.reshape((1, pic_2.shape[0], pic_2.shape[1], pic_2.shape[2])).cuda()\n",
        "pic_3_path = \"/content/hw3_data/p1_data/val/31_4838.jpg\"\n",
        "pic_3 = v_tfm(Image.open(pic_3_path).convert(\"RGB\"))\n",
        "pic_3 = pic_3.reshape((1, pic_3.shape[0], pic_3.shape[1], pic_3.shape[2])).cuda()\n",
        "\n",
        "\n",
        "model = timm.create_model(\"vit_small_patch16_384\", num_classes=37).cuda()\n",
        "model.forward = MethodType(new_forward, model)\n",
        "model.forward_features = MethodType(new_forward_features, model)\n",
        "model.blocks[-1].forward = MethodType(block_forward, model.blocks[-1])\n",
        "model.blocks[-1].attn.forward = MethodType(attn_forward, model.blocks[-1].attn)\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/Hw3/model.ckpt\"))\n",
        "model.eval()\n",
        "print(\"over\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oam7phfXZwni"
      },
      "source": [
        "att_mat = model(pic_1)\n",
        "att_mat = att_mat.reshape((6, 577, 577))\n",
        "att_mat = att_mat.mean(dim=0).cpu().detach().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgYus_xHuODV"
      },
      "source": [
        "visualize_grid_to_grid_with_cls(att_mat, 0, Image.open(pic_1_path).convert(\"RGB\").resize((384, 384)), grid_size=24, alpha=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Ue7IusGrl4N"
      },
      "source": [
        "embed = embed.reshape((embed.shape[1], embed.shape[2]))[1:].cpu().detach().numpy()\n",
        "norm_embed = np.matmul(embed, embed.transpose())\n",
        "norm_embed /= np.linalg.norm(norm_embed)\n",
        "norm_embed = np.reshape(norm_embed, (24, 24, 24*24))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmP6M-WbucmH"
      },
      "source": [
        "total_cos_img = []\n",
        "for o_row in range(24):\n",
        "    total_cos_img.append([])\n",
        "    for o_col in range(24):\n",
        "        cos_img = []\n",
        "        for row in range(24):\n",
        "            cos_img.append([])\n",
        "            for col in range(24):\n",
        "                cos_img[row].append(cosine_similarity(norm_embed[o_row][o_col], norm_embed[row][col]))\n",
        "        cos_img = np.array(cos_img)\n",
        "        total_cos_img[o_row].append(cos_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBk6K1rVH_Sv"
      },
      "source": [
        "fig = plt.figure(figsize=(12, 12))\n",
        "i = 1\n",
        "for row in range(24):\n",
        "    for col in range(24):\n",
        "        fig.add_subplot(24, 24, i)\n",
        "        plt.imshow(total_cos_img[row][col])\n",
        "        plt.axis('off')\n",
        "        i += 1\n",
        "plt.show()        "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}