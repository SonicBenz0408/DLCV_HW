{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DLCV_HW1-1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1KtmCPJ_vM2"
      },
      "source": [
        "! git clone https://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@github.com/DLCV-Fall-2021/hw1-SonicBenz0408.git\n",
        "! bash hw1-SonicBenz0408/get_dataset.sh\n",
        "\n",
        "# Import packages.\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models.utils import load_state_dict_from_url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2xsGeds_yM6"
      },
      "source": [
        "batch_size = 64\n",
        "\n",
        "train_tfm = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.RandomHorizontalFlip(0.5),\n",
        "    transforms.RandomAffine(degrees=(0, 20), translate=(0.1, 0.1)),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.3),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "test_tfm = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "class ImgDataset(Dataset):\n",
        "    def __init__(self, fnames, transform):\n",
        "        self.fnames = fnames\n",
        "        self.transform = transform\n",
        "        self.num_samples = len(self.fnames)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        fname = self.fnames[idx]\n",
        "        # 1. Load the image\n",
        "        img = torchvision.io.read_image(fname)\n",
        "        img = self.transform(img)\n",
        "        for i in range(len(fname)-1, 0, -1):\n",
        "            if fname[i] == \"/\":\n",
        "                if fname[i+2] == \"_\":\n",
        "                    label = int(fname[i+1])\n",
        "                else:\n",
        "                    label = int(fname[i+1:i+3])\n",
        "                break\n",
        "\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "data_path = \"hw1_data/p1_data/\"\n",
        "\n",
        "train_fnames = glob.glob(os.path.join(os.path.join(data_path, \"train_50\"), '*'))\n",
        "val_fnames = glob.glob(os.path.join(os.path.join(data_path, \"val_50\"), '*'))\n",
        "train_set = ImgDataset(train_fnames, train_tfm)\n",
        "train_set, val_set = torch.utils.data.random_split(train_set, [20000, 2500])\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmn0qA1WSGo2"
      },
      "source": [
        "# fix random seed\n",
        "def same_seeds(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  \n",
        "    np.random.seed(seed)  \n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "times = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6d7tm9rcw4y"
      },
      "source": [
        "#model = torchvision.models.vgg16(pretrained=True)\n",
        "\n",
        "#pretrained_weight = load_state_dict_from_url('https://download.pytorch.org/models/vgg16-397923af.pth')\n",
        "#pretrained_weight.pop('classifier.0.bias')\n",
        "#pretrained_weight.pop('classifier.0.weight')\n",
        "#pretrained_weight.pop('classifier.3.bias')\n",
        "#pretrained_weight.pop('classifier.3.weight')\n",
        "#pretrained_weight.pop('classifier.6.bias')\n",
        "#pretrained_weight.pop('classifier.6.weight')\n",
        "\n",
        "model = torchvision.models.inception_v3(pretrained=True, aux_logits=False)\n",
        "model.fc = nn.Linear(2048, 50)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)\n",
        "\n",
        "# number of epoch\n",
        "n_epochs = 200\n",
        "freeze_epochs = 20\n",
        "\n",
        "path = \"/content/model/\"\n",
        "save_path = os.path.join(path, \"model\")\n",
        "best_acc = 0.\n",
        "\n",
        "if not os.path.exists(path):\n",
        "    os.mkdir(path)\n",
        "\n",
        "print(\"Start Training!\")\n",
        "\n",
        "#for param in model.parameters():\n",
        "#    param.requires_grad = False\n",
        "#for param in model.classifier.parameters():\n",
        "#    param.requires_grad = True\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "        \n",
        "    # train\n",
        "    model.train()\n",
        "\n",
        "    train_loss = []\n",
        "    train_accs = []\n",
        "\n",
        "    #if epoch == freeze_epochs:\n",
        "    #    for param in model.parameters():\n",
        "    #        param.requires_grad = True\n",
        "    #    for g in optimizer.param_groups:\n",
        "    #        g['lr'] = 0.0001\n",
        "\n",
        "    for batch in train_loader:\n",
        "\n",
        "        imgs, labels = batch\n",
        "        \n",
        "        logits = model(imgs.to(device))\n",
        "        \n",
        "        loss = criterion(logits, labels.to(device))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the gradient norms for stable training.\n",
        "        # grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
        "        train_loss.append(loss.item())\n",
        "        train_accs.append(acc)\n",
        "\n",
        "    train_loss = sum(train_loss) / len(train_loss)\n",
        "    train_acc = sum(train_accs) / len(train_accs)\n",
        "\n",
        "    # print\n",
        "    print(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n",
        "\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "\n",
        "    valid_loss = []\n",
        "    valid_accs = []\n",
        "\n",
        "    for batch in val_loader:\n",
        "\n",
        "        imgs, labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(imgs.to(device))\n",
        "\n",
        "        loss = criterion(logits, labels.to(device))\n",
        "        acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean()\n",
        "        valid_loss.append(loss.item())\n",
        "        valid_accs.append(acc)\n",
        "\n",
        "    valid_loss = sum(valid_loss) / len(valid_loss)\n",
        "    valid_acc = sum(valid_accs) / len(valid_accs)\n",
        "\n",
        "    if valid_acc > best_acc :\n",
        "    \n",
        "        best_acc = valid_acc\n",
        "        print(f\"\\nsave model with acc = {best_acc:.5f}\")\n",
        "        torch.save(model.state_dict(), save_path + \".ckpt\")\n",
        "\n",
        "    # print\n",
        "    print(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQv2IRh1ai6s"
      },
      "source": [
        "import warnings\n",
        "from collections import namedtuple\n",
        "from typing import Callable, Any, Optional, Tuple, List\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn, Tensor\n",
        "\n",
        "\n",
        "__all__ = [\"Inception3\", \"inception_v3\", \"InceptionOutputs\", \"_InceptionOutputs\"]\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    # Inception v3 ported from TensorFlow\n",
        "    \"inception_v3_google\": \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\",\n",
        "}\n",
        "\n",
        "InceptionOutputs = namedtuple(\"InceptionOutputs\", [\"logits\", \"aux_logits\"])\n",
        "InceptionOutputs.__annotations__ = {\"logits\": Tensor, \"aux_logits\": Optional[Tensor]}\n",
        "\n",
        "# Script annotations failed with _GoogleNetOutputs = namedtuple ...\n",
        "# _InceptionOutputs set here for backwards compat\n",
        "_InceptionOutputs = InceptionOutputs\n",
        "\n",
        "\n",
        "def inception_v3(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> \"Inception3\":\n",
        "    r\"\"\"Inception v3 model architecture from\n",
        "    `\"Rethinking the Inception Architecture for Computer Vision\" <http://arxiv.org/abs/1512.00567>`_.\n",
        "    The required minimum input size of the model is 75x75.\n",
        "\n",
        "    .. note::\n",
        "        **Important**: In contrast to the other models the inception_v3 expects tensors with a size of\n",
        "        N x 3 x 299 x 299, so ensure your images are sized accordingly.\n",
        "\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "        aux_logits (bool): If True, add an auxiliary branch that can improve training.\n",
        "            Default: *True*\n",
        "        transform_input (bool): If True, preprocesses the input according to the method with which it\n",
        "            was trained on ImageNet. Default: *False*\n",
        "    \"\"\"\n",
        "    if pretrained:\n",
        "        if \"transform_input\" not in kwargs:\n",
        "            kwargs[\"transform_input\"] = True\n",
        "        if \"aux_logits\" in kwargs:\n",
        "            original_aux_logits = kwargs[\"aux_logits\"]\n",
        "            kwargs[\"aux_logits\"] = True\n",
        "        else:\n",
        "            original_aux_logits = True\n",
        "        kwargs[\"init_weights\"] = False  # we are loading weights from a pretrained model\n",
        "        model = Inception3(**kwargs)\n",
        "        state_dict = load_state_dict_from_url(model_urls[\"inception_v3_google\"], progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "        if not original_aux_logits:\n",
        "            model.aux_logits = False\n",
        "            model.AuxLogits = None\n",
        "        return model\n",
        "\n",
        "    return Inception3(**kwargs)\n",
        "\n",
        "\n",
        "class Inception3(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int = 1000,\n",
        "        aux_logits: bool = True,\n",
        "        transform_input: bool = False,\n",
        "        inception_blocks: Optional[List[Callable[..., nn.Module]]] = None,\n",
        "        init_weights: Optional[bool] = None,\n",
        "        dropout: float = 0.5,\n",
        "    ) -> None:\n",
        "        super(Inception3, self).__init__()\n",
        "        if inception_blocks is None:\n",
        "            inception_blocks = [BasicConv2d, InceptionA, InceptionB, InceptionC, InceptionD, InceptionE, InceptionAux]\n",
        "        if init_weights is None:\n",
        "            warnings.warn(\n",
        "                \"The default weight initialization of inception_v3 will be changed in future releases of \"\n",
        "                \"torchvision. If you wish to keep the old behavior (which leads to long initialization times\"\n",
        "                \" due to scipy/scipy#11299), please set init_weights=True.\",\n",
        "                FutureWarning,\n",
        "            )\n",
        "            init_weights = True\n",
        "        assert len(inception_blocks) == 7\n",
        "        conv_block = inception_blocks[0]\n",
        "        inception_a = inception_blocks[1]\n",
        "        inception_b = inception_blocks[2]\n",
        "        inception_c = inception_blocks[3]\n",
        "        inception_d = inception_blocks[4]\n",
        "        inception_e = inception_blocks[5]\n",
        "        inception_aux = inception_blocks[6]\n",
        "\n",
        "        self.aux_logits = aux_logits\n",
        "        self.transform_input = transform_input\n",
        "        self.Conv2d_1a_3x3 = conv_block(3, 32, kernel_size=3, stride=2)\n",
        "        self.Conv2d_2a_3x3 = conv_block(32, 32, kernel_size=3)\n",
        "        self.Conv2d_2b_3x3 = conv_block(32, 64, kernel_size=3, padding=1)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.Conv2d_3b_1x1 = conv_block(64, 80, kernel_size=1)\n",
        "        self.Conv2d_4a_3x3 = conv_block(80, 192, kernel_size=3)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.Mixed_5b = inception_a(192, pool_features=32)\n",
        "        self.Mixed_5c = inception_a(256, pool_features=64)\n",
        "        self.Mixed_5d = inception_a(288, pool_features=64)\n",
        "        self.Mixed_6a = inception_b(288)\n",
        "        self.Mixed_6b = inception_c(768, channels_7x7=128)\n",
        "        self.Mixed_6c = inception_c(768, channels_7x7=160)\n",
        "        self.Mixed_6d = inception_c(768, channels_7x7=160)\n",
        "        self.Mixed_6e = inception_c(768, channels_7x7=192)\n",
        "        self.AuxLogits: Optional[nn.Module] = None\n",
        "        if aux_logits:\n",
        "            self.AuxLogits = inception_aux(768, num_classes)\n",
        "        self.Mixed_7a = inception_d(768)\n",
        "        self.Mixed_7b = inception_e(1280)\n",
        "        self.Mixed_7c = inception_e(2048)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.fc = nn.Linear(2048, num_classes)\n",
        "        if init_weights:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                    stddev = float(m.stddev) if hasattr(m, \"stddev\") else 0.1  # type: ignore\n",
        "                    torch.nn.init.trunc_normal_(m.weight, mean=0.0, std=stddev, a=-2, b=2)\n",
        "                elif isinstance(m, nn.BatchNorm2d):\n",
        "                    nn.init.constant_(m.weight, 1)\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _transform_input(self, x: Tensor) -> Tensor:\n",
        "        if self.transform_input:\n",
        "            x_ch0 = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
        "            x_ch1 = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
        "            x_ch2 = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
        "            x = torch.cat((x_ch0, x_ch1, x_ch2), 1)\n",
        "        return x\n",
        "\n",
        "    def _forward(self, x: Tensor) -> Tuple[Tensor, Optional[Tensor]]:\n",
        "        # N x 3 x 299 x 299\n",
        "        x = self.Conv2d_1a_3x3(x)\n",
        "        # N x 32 x 149 x 149\n",
        "        x = self.Conv2d_2a_3x3(x)\n",
        "        # N x 32 x 147 x 147\n",
        "        x = self.Conv2d_2b_3x3(x)\n",
        "        # N x 64 x 147 x 147\n",
        "        x = self.maxpool1(x)\n",
        "        # N x 64 x 73 x 73\n",
        "        x = self.Conv2d_3b_1x1(x)\n",
        "        # N x 80 x 73 x 73\n",
        "        x = self.Conv2d_4a_3x3(x)\n",
        "        # N x 192 x 71 x 71\n",
        "        x = self.maxpool2(x)\n",
        "        # N x 192 x 35 x 35\n",
        "        x = self.Mixed_5b(x)\n",
        "        # N x 256 x 35 x 35\n",
        "        x = self.Mixed_5c(x)\n",
        "        # N x 288 x 35 x 35\n",
        "        x = self.Mixed_5d(x)\n",
        "        # N x 288 x 35 x 35\n",
        "        x = self.Mixed_6a(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.Mixed_6b(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.Mixed_6c(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.Mixed_6d(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.Mixed_6e(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        aux: Optional[Tensor] = None\n",
        "        if self.AuxLogits is not None:\n",
        "            if self.training:\n",
        "                aux = self.AuxLogits(x)\n",
        "        # N x 768 x 17 x 17\n",
        "        x = self.Mixed_7a(x)\n",
        "        # N x 1280 x 8 x 8\n",
        "        x = self.Mixed_7b(x)\n",
        "        # N x 2048 x 8 x 8\n",
        "        x = self.Mixed_7c(x)\n",
        "        # N x 2048 x 8 x 8\n",
        "        # Adaptive average pooling\n",
        "        x = self.avgpool(x)\n",
        "        # N x 2048 x 1 x 1\n",
        "        x = self.dropout(x)\n",
        "        # N x 2048 x 1 x 1\n",
        "        x = torch.flatten(x, 1)\n",
        "        # N x 2048\n",
        "        return x, aux\n",
        "\n",
        "    @torch.jit.unused\n",
        "    def eager_outputs(self, x: Tensor, aux: Optional[Tensor]) -> InceptionOutputs:\n",
        "        if self.training and self.aux_logits:\n",
        "            return InceptionOutputs(x, aux)\n",
        "        else:\n",
        "            return x  # type: ignore[return-value]\n",
        "\n",
        "    def forward(self, x: Tensor) -> InceptionOutputs:\n",
        "        x = self._transform_input(x)\n",
        "        x, aux = self._forward(x)\n",
        "        aux_defined = self.training and self.aux_logits\n",
        "        if torch.jit.is_scripting():\n",
        "            if not aux_defined:\n",
        "                warnings.warn(\"Scripted Inception3 always returns Inception3 Tuple\")\n",
        "            return InceptionOutputs(x, aux)\n",
        "        else:\n",
        "            return self.eager_outputs(x, aux)\n",
        "\n",
        "\n",
        "class InceptionA(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels: int, pool_features: int, conv_block: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(InceptionA, self).__init__()\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "        self.branch1x1 = conv_block(in_channels, 64, kernel_size=1)\n",
        "\n",
        "        self.branch5x5_1 = conv_block(in_channels, 48, kernel_size=1)\n",
        "        self.branch5x5_2 = conv_block(48, 64, kernel_size=5, padding=2)\n",
        "\n",
        "        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n",
        "        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n",
        "        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, padding=1)\n",
        "\n",
        "        self.branch_pool = conv_block(in_channels, pool_features, kernel_size=1)\n",
        "\n",
        "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch5x5 = self.branch5x5_1(x)\n",
        "        branch5x5 = self.branch5x5_2(branch5x5)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionB(nn.Module):\n",
        "    def __init__(self, in_channels: int, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:\n",
        "        super(InceptionB, self).__init__()\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "        self.branch3x3 = conv_block(in_channels, 384, kernel_size=3, stride=2)\n",
        "\n",
        "        self.branch3x3dbl_1 = conv_block(in_channels, 64, kernel_size=1)\n",
        "        self.branch3x3dbl_2 = conv_block(64, 96, kernel_size=3, padding=1)\n",
        "        self.branch3x3dbl_3 = conv_block(96, 96, kernel_size=3, stride=2)\n",
        "\n",
        "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
        "        branch3x3 = self.branch3x3(x)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "\n",
        "        outputs = [branch3x3, branch3x3dbl, branch_pool]\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionC(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels: int, channels_7x7: int, conv_block: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(InceptionC, self).__init__()\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "        self.branch1x1 = conv_block(in_channels, 192, kernel_size=1)\n",
        "\n",
        "        c7 = channels_7x7\n",
        "        self.branch7x7_1 = conv_block(in_channels, c7, kernel_size=1)\n",
        "        self.branch7x7_2 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
        "        self.branch7x7_3 = conv_block(c7, 192, kernel_size=(7, 1), padding=(3, 0))\n",
        "\n",
        "        self.branch7x7dbl_1 = conv_block(in_channels, c7, kernel_size=1)\n",
        "        self.branch7x7dbl_2 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
        "        self.branch7x7dbl_3 = conv_block(c7, c7, kernel_size=(1, 7), padding=(0, 3))\n",
        "        self.branch7x7dbl_4 = conv_block(c7, c7, kernel_size=(7, 1), padding=(3, 0))\n",
        "        self.branch7x7dbl_5 = conv_block(c7, 192, kernel_size=(1, 7), padding=(0, 3))\n",
        "\n",
        "        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n",
        "\n",
        "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch7x7 = self.branch7x7_1(x)\n",
        "        branch7x7 = self.branch7x7_2(branch7x7)\n",
        "        branch7x7 = self.branch7x7_3(branch7x7)\n",
        "\n",
        "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
        "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
        "\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionD(nn.Module):\n",
        "    def __init__(self, in_channels: int, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:\n",
        "        super(InceptionD, self).__init__()\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "        self.branch3x3_1 = conv_block(in_channels, 192, kernel_size=1)\n",
        "        self.branch3x3_2 = conv_block(192, 320, kernel_size=3, stride=2)\n",
        "\n",
        "        self.branch7x7x3_1 = conv_block(in_channels, 192, kernel_size=1)\n",
        "        self.branch7x7x3_2 = conv_block(192, 192, kernel_size=(1, 7), padding=(0, 3))\n",
        "        self.branch7x7x3_3 = conv_block(192, 192, kernel_size=(7, 1), padding=(3, 0))\n",
        "        self.branch7x7x3_4 = conv_block(192, 192, kernel_size=3, stride=2)\n",
        "\n",
        "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = self.branch3x3_2(branch3x3)\n",
        "\n",
        "        branch7x7x3 = self.branch7x7x3_1(x)\n",
        "        branch7x7x3 = self.branch7x7x3_2(branch7x7x3)\n",
        "        branch7x7x3 = self.branch7x7x3_3(branch7x7x3)\n",
        "        branch7x7x3 = self.branch7x7x3_4(branch7x7x3)\n",
        "\n",
        "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "        outputs = [branch3x3, branch7x7x3, branch_pool]\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionE(nn.Module):\n",
        "    def __init__(self, in_channels: int, conv_block: Optional[Callable[..., nn.Module]] = None) -> None:\n",
        "        super(InceptionE, self).__init__()\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "        self.branch1x1 = conv_block(in_channels, 320, kernel_size=1)\n",
        "\n",
        "        self.branch3x3_1 = conv_block(in_channels, 384, kernel_size=1)\n",
        "        self.branch3x3_2a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
        "        self.branch3x3_2b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
        "\n",
        "        self.branch3x3dbl_1 = conv_block(in_channels, 448, kernel_size=1)\n",
        "        self.branch3x3dbl_2 = conv_block(448, 384, kernel_size=3, padding=1)\n",
        "        self.branch3x3dbl_3a = conv_block(384, 384, kernel_size=(1, 3), padding=(0, 1))\n",
        "        self.branch3x3dbl_3b = conv_block(384, 384, kernel_size=(3, 1), padding=(1, 0))\n",
        "\n",
        "        self.branch_pool = conv_block(in_channels, 192, kernel_size=1)\n",
        "\n",
        "    def _forward(self, x: Tensor) -> List[Tensor]:\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "            self.branch3x3_2a(branch3x3),\n",
        "            self.branch3x3_2b(branch3x3),\n",
        "        ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = [\n",
        "            self.branch3x3dbl_3a(branch3x3dbl),\n",
        "            self.branch3x3dbl_3b(branch3x3dbl),\n",
        "        ]\n",
        "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
        "\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        outputs = self._forward(x)\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class InceptionAux(nn.Module):\n",
        "    def __init__(\n",
        "        self, in_channels: int, num_classes: int, conv_block: Optional[Callable[..., nn.Module]] = None\n",
        "    ) -> None:\n",
        "        super(InceptionAux, self).__init__()\n",
        "        if conv_block is None:\n",
        "            conv_block = BasicConv2d\n",
        "        self.conv0 = conv_block(in_channels, 128, kernel_size=1)\n",
        "        self.conv1 = conv_block(128, 768, kernel_size=5)\n",
        "        self.conv1.stddev = 0.01  # type: ignore[assignment]\n",
        "        self.fc = nn.Linear(768, num_classes)\n",
        "        self.fc.stddev = 0.001  # type: ignore[assignment]\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        # N x 768 x 17 x 17\n",
        "        x = F.avg_pool2d(x, kernel_size=5, stride=3)\n",
        "        # N x 768 x 5 x 5\n",
        "        x = self.conv0(x)\n",
        "        # N x 128 x 5 x 5\n",
        "        x = self.conv1(x)\n",
        "        # N x 768 x 1 x 1\n",
        "        # Adaptive average pooling\n",
        "        x = F.adaptive_avg_pool2d(x, (1, 1))\n",
        "        # N x 768 x 1 x 1\n",
        "        x = torch.flatten(x, 1)\n",
        "        # N x 768\n",
        "        x = self.fc(x)\n",
        "        # N x 1000\n",
        "        return x\n",
        "\n",
        "\n",
        "class BasicConv2d(nn.Module):\n",
        "    def __init__(self, in_channels: int, out_channels: int, **kwargs: Any) -> None:\n",
        "        super(BasicConv2d, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        return F.relu(x, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3MFd4PL2vQO"
      },
      "source": [
        "model_dict = torch.load(model_path)\n",
        "model_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8qKmWHzDykw"
      },
      "source": [
        "val_fnames = []\n",
        "for i in range(50):\n",
        "    for j in range(450, 500):\n",
        "        val_fnames.append(os.path.join(data_path, \"val_50/\" + str(i) + \"_\" + str(j) + \".png\"))\n",
        "        \n",
        "test_set = ImgDataset(val_fnames, test_tfm)\n",
        "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = inception_v3(pretrained=True, aux_logits=False).to(device)\n",
        "model.fc = nn.Linear(2048, 50)\n",
        "model_path = \"/content/model.ckpt\"\n",
        "model_dict = torch.load(model_path)\n",
        "model.load_state_dict(model_dict)\n",
        "model.eval()\n",
        "\n",
        "features = np.array(list())\n",
        "for batch in test_loader:\n",
        "    \n",
        "    img, label = batch\n",
        "    with torch.no_grad():\n",
        "        logit = model(img.to(device))\n",
        "    output = logit.cpu().numpy()\n",
        "    if len(features) == 0:\n",
        "        features = output\n",
        "    else:    \n",
        "        features = np.concatenate((features, output))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghCF0eXymGtt"
      },
      "source": [
        "features[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfkY_gl6jBNz"
      },
      "source": [
        "#https://learnopencv.com/t-sne-for-feature-visualization/\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "tsne = TSNE(n_components=2, perplexity=40, verbose=1, init=\"pca\").fit_transform(features)\n",
        "\n",
        "# scale and move the coordinates so they fit [0; 1] range\n",
        "def scale_to_01_range(x):\n",
        "    # compute the distribution range\n",
        "    value_range = (np.max(x) - np.min(x))\n",
        "    # move the distribution so that it starts from zero\n",
        "    # by extracting the minimal value from all its values\n",
        "    starts_from_zero = x - np.min(x)\n",
        "    # make the distribution fit [0; 1] by dividing by its range\n",
        "    return starts_from_zero / value_range"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8VtPhr5uuaB"
      },
      "source": [
        "tsne.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAlpVkw4t05H"
      },
      "source": [
        "# extract x and y coordinates representing the positions of the images on T-SNE plot\n",
        "tx = tsne[:, 0]\n",
        "ty = tsne[:, 1]\n",
        "#tz = tsne[:, 2]\n",
        "\n",
        "tx = scale_to_01_range(tx)\n",
        "ty = scale_to_01_range(ty)\n",
        "#tz = scale_to_01_range(tz)\n",
        "# initialize a matplotlib plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot()\n",
        "# for every class, we'll add a scatter plot separately\n",
        "labels = []\n",
        "cmap = plt.cm.get_cmap(\"nipy_spectral\")\n",
        "colors = cmap(np.linspace(0, 1, 50))\n",
        "for i in range(50):\n",
        "    for j in range(50):\n",
        "        labels.append(i)\n",
        "\n",
        "for i in range(50):\n",
        "    ax.scatter(tx[50*i:50*(i+1)], ty[50*i:50*(i+1)], s=5, color=colors[i], label=str(i))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd_lm1HmhYsY"
      },
      "source": [
        "from statistics import mode\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = inception_v3(pretrained=True, aux_logits=False)\n",
        "model.fc = nn.Linear(2048, 50)\n",
        "model_path = \"/content/model.ckpt\"\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "val_fnames = []\n",
        "for i in range(50):\n",
        "    for j in range(450, 500):\n",
        "        val_fnames.append(os.path.join(data_path, \"val_50/\" + str(i) + \"_\" + str(j) + \".png\"))\n",
        "        \n",
        "test_set = ImgDataset(val_fnames, test_tfm)\n",
        "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZHgTUNkUyFo"
      },
      "source": [
        "model.to(device)\n",
        "\n",
        "count = 0\n",
        "preds = []\n",
        "i = 0\n",
        "for batch in test_loader:\n",
        "    model.eval()\n",
        "    img, label = batch\n",
        "    outputs = []\n",
        "    logit = model(img.to(device))\n",
        "    pred = logit.argmax(dim=-1)\n",
        "    if pred == label.to(device):\n",
        "        count += 1\n",
        "    preds.append(pred.item())\n",
        "    i += 1\n",
        "\n",
        "print(\"over!\")\n",
        "acc = count/len(test_loader)\n",
        "print(\"acc = \", acc)\n",
        "\n",
        "with open(\"./output.csv\", \"w\") as f:\n",
        "    f.write(\"image_id, label\\n\")\n",
        "    index = 0\n",
        "    for i in range(50):\n",
        "        for j in range(450, 500):\n",
        "            string_w = str(i) + \"_\" + str(j) + \".png, \" + str(preds[index]) + \"\\n\"\n",
        "            f.write(string_w)\n",
        "            index += 1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}