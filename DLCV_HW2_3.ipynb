{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "DLCV_HW2-3",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Vi_5DN5pIoF"
      },
      "source": [
        "! /opt/bin/nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh3cVe7bvWYw"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7y4wyYdEABR"
      },
      "source": [
        "### Dataset Download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RetEOmCqu-8O"
      },
      "source": [
        "! git clone https://<token>@github.com/DLCV-Fall-2021/hw2-SonicBenz0408.git\n",
        "! bash ./hw2-SonicBenz0408/get_dataset.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjfM46dtmxXj"
      },
      "source": [
        "## Random seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWuecW1imz42"
      },
      "source": [
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def same_seeds(seed):\n",
        "    # Python built-in random module\n",
        "    random.seed(seed)\n",
        "    # Numpy\n",
        "    np.random.seed(seed)\n",
        "    # Torch\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "same_seeds(7414)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCTPz2iRQmwe"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC8RRsX0QhL-"
      },
      "source": [
        "# Training progress bar\n",
        "!pip install -q qqdm\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils import spectral_norm\n",
        "import matplotlib.pyplot as plt\n",
        "from qqdm.notebook import qqdm\n",
        "from PIL import Image\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYjZ_G83_YX4"
      },
      "source": [
        "## Dataset\n",
        "1. Resize the images to (64, 64)\n",
        "1. Linearly map the values from [0, 1] to  [-1, 1].\n",
        "\n",
        "Please refer to [PyTorch official website](https://pytorch.org/vision/stable/transforms.html) for details about different transforms.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ6d0_cr8R26"
      },
      "source": [
        "class ImgDataset(Dataset):\n",
        "    def __init__(self, path, labels, transform):\n",
        "        self.path = path\n",
        "        self.fnames = os.listdir(self.path)\n",
        "        self.fnames.sort()\n",
        "        self.transform = transform\n",
        "        self.num_samples = len(self.fnames)\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        label = int(self.labels[idx])\n",
        "        fname = os.path.join(self.path, self.fnames[idx])\n",
        "        img = Image.open(fname).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGwdVhOKSjLY"
      },
      "source": [
        "### Show some images\n",
        "Note that the values are in the range of [-1, 1], we should shift them to the valid range, [0, 1], to display correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34mVNtHn7cwF"
      },
      "source": [
        "train_tfm = transforms.Compose([\n",
        "    transforms.ColorJitter(brightness=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "])\n",
        "test_tfm = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
        "])\n",
        "#SVHN----------------------------------------------------------------------\n",
        "s_train_label_path = \"/content/hw2_data/digits/svhn/train.csv\"\n",
        "s_train_label = []\n",
        "with open(s_train_label_path, \"r\") as f:\n",
        "    rows = csv.reader(f)\n",
        "    for row in rows:\n",
        "        s_train_label.append(row[-1])\n",
        "s_train_label.pop(0)\n",
        "\n",
        "s_train_path = \"/content/hw2_data/digits/svhn/train/\"\n",
        "s_train_set = ImgDataset(s_train_path, s_train_label, train_tfm)\n",
        "#MNIST----------------------------------------------------------------------\n",
        "m_train_label_path = \"/content/hw2_data/digits/mnistm/train.csv\"\n",
        "m_train_label = []\n",
        "with open(m_train_label_path, \"r\") as f:\n",
        "    rows = csv.reader(f)\n",
        "    for row in rows:\n",
        "        m_train_label.append(row[-1])\n",
        "m_train_label.pop(0)\n",
        "\n",
        "m_train_path = \"/content/hw2_data/digits/mnistm/train/\"\n",
        "m_train_set = ImgDataset(m_train_path, m_train_label, train_tfm)\n",
        "#USPS----------------------------------------------------------------------\n",
        "u_train_label_path = \"/content/hw2_data/digits/usps/train.csv\"\n",
        "u_train_label = []\n",
        "with open(u_train_label_path, \"r\") as f:\n",
        "    rows = csv.reader(f)\n",
        "    for row in rows:\n",
        "        u_train_label.append(row[-1])\n",
        "u_train_label.pop(0)\n",
        "\n",
        "u_train_path = \"/content/hw2_data/digits/usps/train/\"\n",
        "u_train_set = ImgDataset(u_train_path, u_train_label, train_tfm)\n",
        "\n",
        "#SVHN----------------------------------------------------------------------\n",
        "s_test_label_path = \"/content/hw2_data/digits/svhn/test.csv\"\n",
        "s_test_label = []\n",
        "with open(s_test_label_path, \"r\") as f:\n",
        "    rows = csv.reader(f)\n",
        "    for row in rows:\n",
        "        s_test_label.append(row[-1])\n",
        "s_test_label.pop(0)\n",
        "\n",
        "s_test_path = \"/content/hw2_data/digits/svhn/test/\"\n",
        "s_test_set = ImgDataset(s_test_path, s_test_label, test_tfm)\n",
        "#MNIST----------------------------------------------------------------------\n",
        "m_test_label_path = \"/content/hw2_data/digits/mnistm/test.csv\"\n",
        "m_test_label = []\n",
        "with open(m_test_label_path, \"r\") as f:\n",
        "    rows = csv.reader(f)\n",
        "    for row in rows:\n",
        "        m_test_label.append(row[-1])\n",
        "m_test_label.pop(0)\n",
        "\n",
        "m_test_path = \"/content/hw2_data/digits/mnistm/test/\"\n",
        "m_test_set = ImgDataset(m_test_path, m_test_label, test_tfm)\n",
        "#USPS----------------------------------------------------------------------\n",
        "u_test_label_path = \"/content/hw2_data/digits/usps/test.csv\"\n",
        "u_test_label = []\n",
        "with open(u_test_label_path, \"r\") as f:\n",
        "    rows = csv.reader(f)\n",
        "    for row in rows:\n",
        "        u_test_label.append(row[-1])\n",
        "u_test_label.pop(0)\n",
        "\n",
        "u_test_path = \"/content/hw2_data/digits/usps/test/\"\n",
        "u_test_set = ImgDataset(u_test_path, u_test_label, test_tfm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhxUjRUuHdti"
      },
      "source": [
        "images = [(u_train_set[i][0]+1)/2 for i in range(100)]\n",
        "grid_img = torchvision.utils.make_grid(images, nrow=10)\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(grid_img.permute(1, 2, 0))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZwLQB5Hx1Zo"
      },
      "source": [
        "# SVHN -> MNIST-M"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-kGJ0dG8SZN"
      },
      "source": [
        "## My model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq3pgke6q7Ax"
      },
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(128, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, ceil_mode=True),\n",
        "\n",
        "            nn.Conv2d(256, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        #print(x.shape)\n",
        "        x = x.squeeze()\n",
        "        return x\n",
        "\n",
        "class LabelPredictor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LabelPredictor, self).__init__()\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(1024, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, h):\n",
        "        c = self.layer(h)\n",
        "        return c\n",
        "\n",
        "class DomainClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DomainClassifier, self).__init__()\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(1024, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, h):\n",
        "        y = self.layer(h)\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L15SXEAL_IUU"
      },
      "source": [
        "from torch.autograd import Function\n",
        "class ReverseLayerF(Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, lamb):\n",
        "        ctx.lamb = lamb\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.lamb\n",
        "\n",
        "        return output, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqJ3xccUt8iw"
      },
      "source": [
        "print(domain_classifier)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxo4teqaO5RJ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2EqomOouHezf"
      },
      "source": [
        "batch_size = 32\n",
        "lr = 1e-3\n",
        "n_epoch = 100\n",
        "\n",
        "source_loader = DataLoader(s_train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "target_loader = DataLoader(m_train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "feature_extractor = FeatureExtractor().cuda()\n",
        "label_predictor = LabelPredictor().cuda()\n",
        "domain_classifier = DomainClassifier().cuda()\n",
        "\n",
        "class_criterion = nn.CrossEntropyLoss()\n",
        "domain_criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer_F = optim.Adam(feature_extractor.parameters(), lr=lr)\n",
        "optimizer_C = optim.Adam(label_predictor.parameters(), lr=lr)\n",
        "optimizer_D = optim.Adam(domain_classifier.parameters(), lr=lr)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GeDkuhR0sL8"
      },
      "source": [
        "loss_list, acc_list = [], []\n",
        "label_predictor.train()\n",
        "feature_extractor.train()\n",
        "domain_classifier.train()\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "    \n",
        "\n",
        "    running_loss = 0.0\n",
        "    s_count, total_num = 0, 0\n",
        "\n",
        "    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_loader, target_loader)):\n",
        "        lamb= 2 / (1 + math.exp(-10*epoch/n_epoch))\n",
        "        bs = source_data.shape[0]\n",
        "\n",
        "        source_data = source_data.cuda()\n",
        "        source_label = source_label.cuda()\n",
        "\n",
        "        s_domain_label = torch.zeros((bs, 1)).cuda()\n",
        "        \n",
        "        s_feature = feature_extractor(source_data)\n",
        "        s_r_feature = ReverseLayerF.apply(s_feature, lamb)\n",
        "        s_domain_logits = domain_classifier(s_r_feature)\n",
        "        s_class_logits = label_predictor(s_feature)\n",
        "\n",
        "        domain_loss_s = domain_criterion(s_domain_logits, s_domain_label)\n",
        "        class_loss_s = class_criterion(s_class_logits, source_label)\n",
        "\n",
        "        bs = target_data.shape[0]\n",
        "\n",
        "        target_data = target_data.cuda()\n",
        "\n",
        "        t_domain_label = torch.ones((bs, 1)).cuda()\n",
        "        \n",
        "        t_feature = feature_extractor(target_data)\n",
        "        t_r_feature = ReverseLayerF.apply(t_feature, lamb)\n",
        "        t_domain_logits = domain_classifier(t_r_feature)\n",
        "\n",
        "        domain_loss_t = domain_criterion(t_domain_logits, t_domain_label)\n",
        "        \n",
        "        loss = domain_loss_s + class_loss_s + domain_loss_t\n",
        "        loss.backward()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        optimizer_D.step()\n",
        "        optimizer_F.step()\n",
        "        optimizer_C.step()\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        optimizer_F.zero_grad()\n",
        "        optimizer_C.zero_grad()\n",
        "        \n",
        "        s_count += torch.sum(torch.argmax(s_class_logits, dim=1) == source_label).item()\n",
        "        total_num += bs\n",
        "        \n",
        "    train_loss = running_loss / (i+1)\n",
        "    train_acc = s_count / total_num\n",
        "    loss_list.append(train_loss)\n",
        "    acc_list.append(train_acc)\n",
        "    torch.save(feature_extractor.state_dict(), f'/content/extractor_model.bin')\n",
        "    torch.save(label_predictor.state_dict(), f'/content/predictor_model.bin')\n",
        "    torch.save(domain_classifier.state_dict(), f'/content/domain_model.bin')\n",
        "    torch.save(optimizer_F.state_dict(), f'/content/F_opt.bin')\n",
        "    torch.save(optimizer_C.state_dict(), f'/content/C_opt.bin')\n",
        "    torch.save(optimizer_D.state_dict(), f'/content/D_opt.bin')\n",
        "    print('epoch {:>3d}: loss: {:6.4f}, acc {:6.4f}'.format(epoch+1, train_loss, train_acc))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IcMDDGkJB9Zz"
      },
      "source": [
        "D_loss_list, F_loss_list, acc_list = [], [], []\n",
        "label_predictor.train()\n",
        "feature_extractor.train()\n",
        "domain_classifier.train()\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "\n",
        "    running_D_loss, running_F_loss = 0.0, 0.0\n",
        "    total_hit, total_num = 0.0, 0.0\n",
        "\n",
        "    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_loader, target_loader)):\n",
        "        lamb= 2 / (1 + math.exp(-10*epoch/n_epoch))\n",
        "\n",
        "        source_data = source_data.cuda()\n",
        "        source_label = source_label.cuda()\n",
        "        target_data = target_data.cuda()\n",
        "        \n",
        "        mixed_data = torch.cat([source_data, target_data], dim=0)\n",
        "        domain_label = torch.zeros([source_data.shape[0] + target_data.shape[0], 1]).cuda()\n",
        "        domain_label[:source_data.shape[0]] = 1\n",
        "\n",
        "        feature = feature_extractor(mixed_data)\n",
        "        domain_logits = domain_classifier(feature.detach())\n",
        "        loss = domain_criterion(domain_logits, domain_label)\n",
        "        running_D_loss+= loss.item()\n",
        "        loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        class_logits = label_predictor(feature[:source_data.shape[0]])\n",
        "        domain_logits = domain_classifier(feature)\n",
        "        \n",
        "        loss = class_criterion(class_logits, source_label) - lamb * domain_criterion(domain_logits, domain_label)\n",
        "        running_F_loss+= loss.item()\n",
        "        loss.backward()\n",
        "        optimizer_F.step()\n",
        "        optimizer_C.step()\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        optimizer_F.zero_grad()\n",
        "        optimizer_C.zero_grad()\n",
        "\n",
        "        total_hit += torch.sum(torch.argmax(class_logits, dim=1) == source_label).item()\n",
        "        total_num += source_data.shape[0]\n",
        "        print(i, end='\\r')\n",
        "\n",
        "    train_D_loss, train_F_loss, train_acc = running_D_loss / (i+1), running_F_loss / (i+1), total_hit / total_num\n",
        "\n",
        "    D_loss_list.append(train_D_loss)\n",
        "    F_loss_list.append(train_F_loss)\n",
        "    acc_list.append(train_acc)\n",
        "    torch.save(feature_extractor.state_dict(), f'/content/drive/MyDrive/Hw2/2-3/b_s2m_E.bin')\n",
        "    torch.save(label_predictor.state_dict(), f'/content/drive/MyDrive/Hw2/2-3/b_s2m_P.bin')\n",
        "    torch.save(domain_classifier.state_dict(), f'/content/drive/MyDrive/Hw2/2-3/b_s2m_D.bin')\n",
        "    \n",
        "    print('epoch {:>3d}: train D loss: {:6.4f}, train F loss: {:6.4f}, acc {:6.4f}'.format(epoch, train_D_loss, train_F_loss, train_acc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKRgSYbiLwNU"
      },
      "source": [
        "label_predictor = LabelPredictor().cuda()\n",
        "feature_extractor = FeatureExtractor().cuda()\n",
        "\n",
        "label_predictor.load_state_dict(torch.load(f'/content/drive/MyDrive/Hw2/2-3/b_s2m_P.bin'))\n",
        "feature_extractor.load_state_dict(torch.load(f'/content/drive/MyDrive/Hw2/2-3/b_s2m_E.bin'))\n",
        "\n",
        "label_predictor.eval()\n",
        "feature_extractor.eval()\n",
        "\n",
        "test_loader = DataLoader(m_test_set, batch_size=128, shuffle=False)\n",
        "count = 0\n",
        "total = len(m_test_set)\n",
        "for i, (data, label) in enumerate(test_loader):\n",
        "    data, label = data.cuda(), label.cuda()\n",
        "\n",
        "    logit = label_predictor(feature_extractor(data))\n",
        "    pred = logit.argmax(dim=-1)\n",
        "    count += torch.sum(pred==label).item()\n",
        "\n",
        "print(\"acc = \", count / total)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vl-eGsIq1nS-"
      },
      "source": [
        "# MNIST-M -> USPS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_oT87u21nS_"
      },
      "source": [
        "## My model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML2GCwnD1nS_"
      },
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(128, 128, 3, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, ceil_mode=True),\n",
        "\n",
        "            nn.Conv2d(256, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.squeeze()\n",
        "        return x\n",
        "\n",
        "class LabelPredictor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LabelPredictor, self).__init__()\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(1024, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, h):\n",
        "        c = self.layer(h)\n",
        "        return c\n",
        "\n",
        "class DomainClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DomainClassifier, self).__init__()\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(1024, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, h):\n",
        "        y = self.layer(h)\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAdMIMPr1nS_"
      },
      "source": [
        "from torch.autograd import Function\n",
        "class ReverseLayerF(Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, lamb):\n",
        "        ctx.lamb = lamb\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.lamb\n",
        "\n",
        "        return output, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg4ZKOgY1nS_"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pp5gtep91nS_"
      },
      "source": [
        "batch_size = 32\n",
        "lr = 1e-3\n",
        "n_epoch = 200\n",
        "\n",
        "source_loader = DataLoader(m_train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "target_loader = DataLoader(u_train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "feature_extractor = FeatureExtractor().cuda()\n",
        "label_predictor = LabelPredictor().cuda()\n",
        "domain_classifier = DomainClassifier().cuda()\n",
        "\n",
        "class_criterion = nn.CrossEntropyLoss()\n",
        "domain_criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer_F = optim.Adam(feature_extractor.parameters(), lr=lr)\n",
        "optimizer_C = optim.Adam(label_predictor.parameters(), lr=lr)\n",
        "optimizer_D = optim.Adam(domain_classifier.parameters(), lr=lr)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EBzZ6fr1nTA"
      },
      "source": [
        "loss_list, acc_list = [], []\n",
        "label_predictor.train()\n",
        "feature_extractor.train()\n",
        "domain_classifier.train()\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "    \n",
        "\n",
        "    running_loss = 0.0\n",
        "    s_count, total_num = 0, 0\n",
        "\n",
        "    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_loader, target_loader)):\n",
        "        lamb= 2 / (1 + math.exp(-10*epoch/n_epoch))\n",
        "        bs = source_data.shape[0]\n",
        "\n",
        "        source_data = source_data.cuda()\n",
        "        source_label = source_label.cuda()\n",
        "\n",
        "        s_domain_label = torch.zeros((bs, 1)).cuda()\n",
        "        \n",
        "        s_feature = feature_extractor(source_data)\n",
        "        s_r_feature = ReverseLayerF.apply(s_feature, lamb)\n",
        "        s_domain_logits = domain_classifier(s_r_feature)\n",
        "        s_class_logits = label_predictor(s_feature)\n",
        "\n",
        "        domain_loss_s = domain_criterion(s_domain_logits, s_domain_label)\n",
        "        class_loss_s = class_criterion(s_class_logits, source_label)\n",
        "\n",
        "        bs = target_data.shape[0]\n",
        "\n",
        "        target_data = target_data.cuda()\n",
        "\n",
        "        t_domain_label = torch.ones((bs, 1)).cuda()\n",
        "        \n",
        "        t_feature = feature_extractor(target_data)\n",
        "        t_r_feature = ReverseLayerF.apply(t_feature, lamb)\n",
        "        t_domain_logits = domain_classifier(t_r_feature)\n",
        "\n",
        "        domain_loss_t = domain_criterion(t_domain_logits, t_domain_label)\n",
        "        \n",
        "        loss = domain_loss_s + class_loss_s + domain_loss_t\n",
        "        loss.backward()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        optimizer_D.step()\n",
        "        optimizer_F.step()\n",
        "        optimizer_C.step()\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        optimizer_F.zero_grad()\n",
        "        optimizer_C.zero_grad()\n",
        "        \n",
        "        s_count += torch.sum(torch.argmax(s_class_logits, dim=1) == source_label).item()\n",
        "        total_num += bs\n",
        "    train_loss = running_loss / (i+1)\n",
        "    train_acc = s_count / total_num\n",
        "    loss_list.append(train_loss)\n",
        "    acc_list.append(train_acc)\n",
        "    torch.save(feature_extractor.state_dict(), f'/content/extractor_model.bin')\n",
        "    torch.save(label_predictor.state_dict(), f'/content/predictor_model.bin')\n",
        "    torch.save(domain_classifier.state_dict(), f'/content/domain_model.bin')\n",
        "    torch.save(optimizer_F.state_dict(), f'/content/F_opt.bin')\n",
        "    torch.save(optimizer_C.state_dict(), f'/content/C_opt.bin')\n",
        "    torch.save(optimizer_D.state_dict(), f'/content/D_opt.bin')\n",
        "    print('epoch {:>3d}: loss: {:6.4f}, acc {:6.4f}'.format(epoch+1, train_loss, train_acc))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOzY2HaGpk7P"
      },
      "source": [
        "D_loss_list, F_loss_list, acc_list = [], [], []\n",
        "label_predictor.train()\n",
        "feature_extractor.train()\n",
        "domain_classifier.train()\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "\n",
        "    running_D_loss, running_F_loss = 0.0, 0.0\n",
        "    total_hit, total_num = 0.0, 0.0\n",
        "\n",
        "    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_loader, target_loader)):\n",
        "        lamb= 2 / (1 + math.exp(-10*epoch/n_epoch))\n",
        "\n",
        "        source_data = source_data.cuda()\n",
        "        source_label = source_label.cuda()\n",
        "        target_data = target_data.cuda()\n",
        "        \n",
        "        mixed_data = torch.cat([source_data, target_data], dim=0)\n",
        "        domain_label = torch.zeros([source_data.shape[0] + target_data.shape[0], 1]).cuda()\n",
        "        domain_label[:source_data.shape[0]] = 1\n",
        "\n",
        "        feature = feature_extractor(mixed_data)\n",
        "        domain_logits = domain_classifier(feature.detach())\n",
        "        loss = domain_criterion(domain_logits, domain_label)\n",
        "        running_D_loss+= loss.item()\n",
        "        loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        class_logits = label_predictor(feature[:source_data.shape[0]])\n",
        "        domain_logits = domain_classifier(feature)\n",
        "\n",
        "        loss = class_criterion(class_logits, source_label) - lamb * domain_criterion(domain_logits, domain_label)\n",
        "        running_F_loss+= loss.item()\n",
        "        loss.backward()\n",
        "        optimizer_F.step()\n",
        "        optimizer_C.step()\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        optimizer_F.zero_grad()\n",
        "        optimizer_C.zero_grad()\n",
        "\n",
        "        total_hit += torch.sum(torch.argmax(class_logits, dim=1) == source_label).item()\n",
        "        total_num += source_data.shape[0]\n",
        "        print(i, end='\\r')\n",
        "\n",
        "    train_D_loss, train_F_loss, train_acc = running_D_loss / (i+1), running_F_loss / (i+1), total_hit / total_num\n",
        "\n",
        "    D_loss_list.append(train_D_loss)\n",
        "    F_loss_list.append(train_F_loss)\n",
        "    acc_list.append(train_acc)\n",
        "    torch.save(feature_extractor.state_dict(), f'/content/drive/MyDrive/Hw2/2-3/b_m2u_E_s.bin')\n",
        "    torch.save(label_predictor.state_dict(), f'/content/drive/MyDrive/Hw2/2-3/b_m2u_P_s.bin')\n",
        "    torch.save(domain_classifier.state_dict(), f'/content/drive/MyDrive/Hw2/2-3/b_m2u_D_s.bin')\n",
        "    print('epoch {:>3d}: train D loss: {:6.4f}, train F loss: {:6.4f}, acc {:6.4f}'.format(epoch, train_D_loss, train_F_loss, train_acc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcfoYryq1nTA"
      },
      "source": [
        "label_predictor.eval()\n",
        "feature_extractor.eval()\n",
        "\n",
        "test_loader = DataLoader(u_test_set, batch_size=128, shuffle=False)\n",
        "count = 0\n",
        "total = len(u_test_set)\n",
        "for i, (data, label) in enumerate(test_loader):\n",
        "    data, label = data.cuda(), label.cuda()\n",
        "\n",
        "    logit = label_predictor(feature_extractor(data))\n",
        "    pred = logit.argmax(dim=-1)\n",
        "    count += torch.sum(pred==label).item()\n",
        "\n",
        "print(\"acc = \", count / total)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py1tX_PmT6z-"
      },
      "source": [
        "# USPS -> SVHN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFPFl9dossDl"
      },
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 5),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 5),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            nn.Conv2d(128, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            \n",
        "            nn.Conv2d(256, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "\n",
        "            nn.Conv2d(512, 512, 4),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.squeeze()\n",
        "        return x\n",
        "\n",
        "class LabelPredictor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LabelPredictor, self).__init__()\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(1024, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, h):\n",
        "        c = self.layer(h)\n",
        "        return c\n",
        "\n",
        "class DomainClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DomainClassifier, self).__init__()\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(512, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(512, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, h):\n",
        "        y = self.layer(h)\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdKHoOtBT6z-"
      },
      "source": [
        "## My model ori\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58n0zxupT6z-"
      },
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, 5),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, 1, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "\n",
        "            nn.Conv2d(64, 128, 5),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "            nn.Conv2d(128, 256, 3, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            \n",
        "            nn.Conv2d(256, 512, 3, 1, 1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            \n",
        "\n",
        "            nn.Conv2d(512, 1024, 4),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = x.squeeze()\n",
        "        return x\n",
        "\n",
        "class LabelPredictor(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(LabelPredictor, self).__init__()\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(1024, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, h):\n",
        "        c = self.layer(h)\n",
        "        return c\n",
        "\n",
        "class DomainClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DomainClassifier, self).__init__()\n",
        "\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(1024, 1024),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.Linear(512, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, h):\n",
        "        y = self.layer(h)\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2daiu_vBT6z_"
      },
      "source": [
        "from torch.autograd import Function\n",
        "class ReverseLayerF(Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, lamb):\n",
        "        ctx.lamb = lamb\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.lamb\n",
        "\n",
        "        return output, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pq4KV6D6T60A"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uMdlIxAT60A"
      },
      "source": [
        "batch_size = 16\n",
        "lr = 1e-3\n",
        "n_epoch = 200\n",
        "\n",
        "source_loader = DataLoader(u_train_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "target_loader = DataLoader(s_train_set, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "\n",
        "feature_extractor = FeatureExtractor().cuda()\n",
        "label_predictor = LabelPredictor().cuda()\n",
        "domain_classifier = DomainClassifier().cuda()\n",
        "\n",
        "class_criterion = nn.CrossEntropyLoss()\n",
        "domain_criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer_F = optim.Adam(feature_extractor.parameters(), lr=lr)\n",
        "optimizer_C = optim.Adam(label_predictor.parameters(), lr=lr)\n",
        "optimizer_D = optim.Adam(domain_classifier.parameters(), lr=lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6Qsq108wNbI"
      },
      "source": [
        "print(domain_classifier)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N73agxTfT60A"
      },
      "source": [
        "loss_list, acc_list = [], []\n",
        "label_predictor.train()\n",
        "feature_extractor.train()\n",
        "domain_classifier.train()\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "    \n",
        "\n",
        "    running_loss = 0.0\n",
        "    s_count, total_num = 0, 0\n",
        "\n",
        "    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_loader, target_loader)):\n",
        "        lamb= 2 / (1 + math.exp(-10*epoch/n_epoch))\n",
        "        bs = source_data.shape[0]\n",
        "\n",
        "        source_data = source_data.cuda()\n",
        "        source_label = source_label.cuda()\n",
        "\n",
        "        s_domain_label = torch.zeros((bs, 1)).cuda()\n",
        "        \n",
        "        s_feature = feature_extractor(source_data)\n",
        "        s_r_feature = ReverseLayerF.apply(s_feature, lamb)\n",
        "        s_domain_logits = domain_classifier(s_r_feature)\n",
        "        s_class_logits = label_predictor(s_feature)\n",
        "\n",
        "        domain_loss_s = domain_criterion(s_domain_logits, s_domain_label)\n",
        "        class_loss_s = class_criterion(s_class_logits, source_label)\n",
        "\n",
        "        bs = target_data.shape[0]\n",
        "\n",
        "        target_data = target_data.cuda()\n",
        "\n",
        "        t_domain_label = torch.ones((bs, 1)).cuda()\n",
        "        \n",
        "        t_feature = feature_extractor(target_data)\n",
        "        t_r_feature = ReverseLayerF.apply(t_feature, lamb)\n",
        "        t_domain_logits = domain_classifier(t_r_feature)\n",
        "\n",
        "        domain_loss_t = domain_criterion(t_domain_logits, t_domain_label)\n",
        "        \n",
        "        loss = domain_loss_s + class_loss_s + domain_loss_t\n",
        "        loss.backward()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        optimizer_D.step()\n",
        "        optimizer_F.step()\n",
        "        optimizer_C.step()\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        optimizer_F.zero_grad()\n",
        "        optimizer_C.zero_grad()\n",
        "        \n",
        "        s_count += torch.sum(torch.argmax(s_class_logits, dim=1) == source_label).item()\n",
        "        total_num += bs\n",
        "    train_loss = running_loss / (i+1)\n",
        "    train_acc = s_count / total_num\n",
        "    loss_list.append(train_loss)\n",
        "    acc_list.append(train_acc)\n",
        "    torch.save(feature_extractor.state_dict(), f'/content/extractor_model.bin')\n",
        "    torch.save(label_predictor.state_dict(), f'/content/predictor_model.bin')\n",
        "    torch.save(domain_classifier.state_dict(), f'/content/domain_model.bin')\n",
        "    torch.save(optimizer_F.state_dict(), f'/content/F_opt.bin')\n",
        "    torch.save(optimizer_C.state_dict(), f'/content/C_opt.bin')\n",
        "    torch.save(optimizer_D.state_dict(), f'/content/D_opt.bin')\n",
        "    print('epoch {:>3d}: loss: {:6.4f}, acc {:6.4f}'.format(epoch+1, train_loss, train_acc))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh8Y3tQdTUxS"
      },
      "source": [
        "D_loss_list, F_loss_list, acc_list = [], [], []\n",
        "label_predictor.train()\n",
        "feature_extractor.train()\n",
        "domain_classifier.train()\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "\n",
        "    running_D_loss, running_F_loss = 0.0, 0.0\n",
        "    total_hit, total_num = 0.0, 0.0\n",
        "\n",
        "    for i, ((source_data, source_label), (target_data, _)) in enumerate(zip(source_loader, target_loader)):\n",
        "        lamb= 2 / (1 + math.exp(-10*epoch/n_epoch))\n",
        "\n",
        "        source_data = source_data.cuda()\n",
        "        source_label = source_label.cuda()\n",
        "        target_data = target_data.cuda()\n",
        "        \n",
        "        mixed_data = torch.cat([source_data, target_data], dim=0)\n",
        "        domain_label = torch.zeros([source_data.shape[0] + target_data.shape[0], 1]).cuda()\n",
        "        domain_label[:source_data.shape[0]] = 1\n",
        "\n",
        "        feature = feature_extractor(mixed_data)\n",
        "        domain_logits = domain_classifier(feature.detach())\n",
        "        loss = domain_criterion(domain_logits, domain_label)\n",
        "        running_D_loss+= loss.item()\n",
        "        loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        class_logits = label_predictor(feature[:source_data.shape[0]])\n",
        "        domain_logits = domain_classifier(feature)\n",
        "        loss = class_criterion(class_logits, source_label) - lamb * domain_criterion(domain_logits, domain_label)\n",
        "        running_F_loss+= loss.item()\n",
        "        loss.backward()\n",
        "        optimizer_F.step()\n",
        "        optimizer_C.step()\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        optimizer_F.zero_grad()\n",
        "        optimizer_C.zero_grad()\n",
        "\n",
        "        total_hit += torch.sum(torch.argmax(class_logits, dim=1) == source_label).item()\n",
        "        total_num += source_data.shape[0]\n",
        "        print(i, end='\\r')\n",
        "\n",
        "    train_D_loss, train_F_loss, train_acc = running_D_loss / (i+1), running_F_loss / (i+1), total_hit / total_num\n",
        "\n",
        "    D_loss_list.append(train_D_loss)\n",
        "    F_loss_list.append(train_F_loss)\n",
        "    acc_list.append(train_acc)\n",
        "    torch.save(feature_extractor.state_dict(), f'/content/drive/MyDrive/Hw2/2-3/u2s_E_t.bin')\n",
        "    torch.save(label_predictor.state_dict(), f'/content/drive/MyDrive/Hw2/2-3/u2s_P_t.bin')\n",
        "    torch.save(domain_classifier.state_dict(), f'/content/drive/MyDrive/Hw2/2-3/u2s_D_t.bin')\n",
        "    print('epoch {:>3d}: train D loss: {:6.4f}, train F loss: {:6.4f}, acc {:6.4f}'.format(epoch, train_D_loss, train_F_loss, train_acc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJi_fohMT60B"
      },
      "source": [
        "label_predictor = LabelPredictor().cuda()\n",
        "feature_extractor = FeatureExtractor().cuda()\n",
        "\n",
        "label_predictor.load_state_dict(torch.load(f'/content/drive/MyDrive/Hw2/2-3/u2s_P_s.bin'))\n",
        "feature_extractor.load_state_dict(torch.load(f'/content/drive/MyDrive/Hw2/2-3/u2s_E_s.bin'))\n",
        "\n",
        "label_predictor.eval()\n",
        "feature_extractor.eval()\n",
        "\n",
        "test_loader = DataLoader(s_test_set, batch_size=128, shuffle=False)\n",
        "count = 0\n",
        "total = len(s_test_set)\n",
        "for i, (data, label) in enumerate(test_loader):\n",
        "    data, label = data.cuda(), label.cuda()\n",
        "\n",
        "    logit = label_predictor(feature_extractor(data))\n",
        "    pred = logit.argmax(dim=-1)\n",
        "    count += torch.sum(pred==label).item()\n",
        "\n",
        "print(\"acc = \", count / total)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLcBMmNKun5T"
      },
      "source": [
        "## TSNE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGZHxEt9VgQy"
      },
      "source": [
        "feature_extractor = FeatureExtractor().cuda()\n",
        "label_predictor = LabelPredictor().cuda()\n",
        "feature_extractor.eval()\n",
        "label_predictor.eval()\n",
        "E_path = \"/content/drive/MyDrive/Hw2/2-3/s2m_E.bin\"\n",
        "P_path = \"/content/drive/MyDrive/Hw2/2-3/s2m_P.bin\"\n",
        "feature_extractor.load_state_dict(torch.load(E_path))\n",
        "label_predictor.load_state_dict(torch.load(P_path))\n",
        "source_loader = DataLoader(s_test_set, batch_size=1, shuffle=False, num_workers=2)\n",
        "target_loader = DataLoader(m_test_set, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "choose_s, choose_m = [], []\n",
        "for img, _ in source_loader:\n",
        "    img = img.cuda()\n",
        "    feature = feature_extractor(img)\n",
        "    feature = feature.view(1, feature.shape[0])\n",
        "    output = label_predictor(feature)\n",
        "    choose_s.append(output.detach().cpu().numpy().flatten())\n",
        "for img, _ in target_loader:\n",
        "    img = img.cuda()\n",
        "    feature = feature_extractor(img)\n",
        "    feature = feature.view(1, feature.shape[0])\n",
        "    output = label_predictor(feature)\n",
        "    choose_m.append(output.detach().cpu().numpy().flatten())\n",
        "choose_s = np.array(choose_s)\n",
        "choose_m = np.array(choose_m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpcq5twTunVF"
      },
      "source": [
        "#choose_s, choose_m, choose_u = [], [], []\n",
        "label_s, label_m, label_u = [], [], []\n",
        "\n",
        "for i in range(len(s_test_set)):\n",
        "    #choose_s.append(s_test_set[i][0].numpy().flatten())\n",
        "    label_s.append(s_test_set[i][1])\n",
        "for i in range(len(m_test_set)):\n",
        "    #choose_m.append(m_test_set[i][0].numpy().flatten())\n",
        "    label_m.append(m_test_set[i][1])\n",
        "for i in range(len(u_test_set)):\n",
        "    #choose_u.append(u_test_set[i][0].numpy().flatten())\n",
        "    label_u.append(u_test_set[i][1])\n",
        "label_s = np.array(label_s)\n",
        "label_m = np.array(label_m)\n",
        "label_u = np.array(label_u)\n",
        "#choose_s = np.array(choose_s)\n",
        "#choose_m = np.array(choose_m)\n",
        "#choose_u = np.array(choose_u)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HqErFQl2Up0"
      },
      "source": [
        "a = np.arange(100)\n",
        "label_s[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhE6Wxs55EI4"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "s_m = np.concatenate((choose_s, choose_m))\n",
        "label = np.concatenate((label_s, label_m))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNWZ72DfxeWb"
      },
      "source": [
        "rows = np.arange(u_s.shape[0])\n",
        "np.random.shuffle(rows)\n",
        "n_select = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCGakIy4u2w8"
      },
      "source": [
        "tsne = TSNE(n_components=2, learning_rate=200, early_exaggeration=12.0, perplexity=50, verbose=1, n_iter=1000).fit_transform(s_m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DABrvOxrJAwO"
      },
      "source": [
        "s_m.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYaCkExB5WI5"
      },
      "source": [
        "# scale and move the coordinates so they fit [0; 1] range\n",
        "def scale_to_01_range(x):\n",
        "    # compute the distribution range\n",
        "    value_range = (np.max(x) - np.min(x))\n",
        "    # move the distribution so that it starts from zero\n",
        "    # by extracting the minimal value from all its values\n",
        "    starts_from_zero = x - np.min(x)\n",
        "    # make the distribution fit [0; 1] by dividing by its range\n",
        "    return starts_from_zero / value_range\n",
        "\n",
        "# extract x and y coordinates representing the positions of the images on T-SNE plot\n",
        "tx = tsne[:, 0]\n",
        "ty = tsne[:, 1]\n",
        "\n",
        "tx = scale_to_01_range(tx)\n",
        "ty = scale_to_01_range(ty)\n",
        "# initialize a matplotlib plot\n",
        "fig,ax=plt.subplots(1, 2, figsize=(12, 5))\n",
        "# for every class, we'll add a scatter plot separately\n",
        "\n",
        "cmap = plt.cm.get_cmap(\"nipy_spectral\")\n",
        "colors = cmap(np.linspace(0, 1, 10))\n",
        "for i in range(10):\n",
        "    ax[0].scatter(tx[label == i], ty[label == i], s=3, color=colors[i], label=str(i))\n",
        "    ax[0].legend(loc=\"best\")\n",
        "    #ax.scatter(tx[label_m == i], ty[label_m == i], s=10, color=colors[i], label=str(i))\n",
        "ax[1].scatter(tx[:choose_s.shape[0]], ty[:choose_s.shape[0]], s=3, color=colors[2], label=\"source\")\n",
        "ax[1].scatter(tx[choose_s.shape[0]:], ty[choose_s.shape[0]:], s=3, color=colors[8], label=\"target\")\n",
        "ax[1].legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}